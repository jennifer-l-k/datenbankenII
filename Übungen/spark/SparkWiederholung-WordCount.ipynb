{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc098ad",
   "metadata": {},
   "source": [
    "# Spark Tutorial - Wiederholung vom 12.12.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8329306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir wollen DataFrames nutzen und daf√ºr brauchen wir eine SparkSession\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .appName(\"Datenbanken mit Spark\")\\\n",
    "    .getOrCreate() # getOrCreate liefert existierende Session, wenn es schon eine gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36218a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://936664aaf5c7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Datenbanken mit Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa8c48575b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61042a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File einlesen\n",
    "frank = spark.read.text(\"../frankenstein.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f96965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe mit einer Spalte\n",
    "frank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60ba31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explizites Anzeigen des Schemas\n",
    "frank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264e2a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|Project Gutenberg...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "|almost no restric...|\n",
      "|re-use it under t...|\n",
      "|with this eBook o...|\n",
      "|                    |\n",
      "|                    |\n",
      "| Title: Frankenstein|\n",
      "|       or The Mod...|\n",
      "|                    |\n",
      "|Author: Mary Woll...|\n",
      "|                    |\n",
      "|Release Date: Jun...|\n",
      "|Last updated: Jan...|\n",
      "|                    |\n",
      "|   Language: English|\n",
      "|                    |\n",
      "|Character set enc...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7d0d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7834"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb506c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         words|\n",
      "+--------------+\n",
      "|              |\n",
      "|       Project|\n",
      "|     Gutenberg|\n",
      "|             s|\n",
      "| Frankenstein,|\n",
      "|            by|\n",
      "|          Mary|\n",
      "|Wollstonecraft|\n",
      "|              |\n",
      "|        Godwin|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = frank.select(F.explode(F.split(frank.value, \"[^a-z,A-Z]\")).alias(\"words\"))\n",
    "words.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea2eb2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|   words_lower|\n",
      "+--------------+\n",
      "|              |\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|             s|\n",
      "| frankenstein,|\n",
      "|            by|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|              |\n",
      "|        godwin|\n",
      "|              |\n",
      "|       shelley|\n",
      "|              |\n",
      "|          this|\n",
      "|         ebook|\n",
      "|            is|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|            of|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_lower = words.select(F.lower(F.col(\"words\")).alias(\"words_lower\"))\n",
    "words_lower.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75e119ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|   words_clean|\n",
      "+--------------+\n",
      "|              |\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|              |\n",
      "|  frankenstein|\n",
      "|              |\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|              |\n",
      "|        godwin|\n",
      "|              |\n",
      "|       shelley|\n",
      "|              |\n",
      "|          this|\n",
      "|         ebook|\n",
      "|             i|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|              |\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_clean = words_lower.select(F.regexp_extract(\"words_lower\", \"[a-zA-z]{3,}|a|i\", 0).alias(\"words_clean\"))\n",
    "words_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6645431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|words_nonempty|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|             i|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|             a|\n",
      "|          cost|\n",
      "|           and|\n",
      "|          with|\n",
      "|        almost|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_nonempty = words_clean.filter(words_clean.words_clean != \"\").withColumnRenamed(\"words_clean\", \"words_nonempty\")\n",
    "words_nonempty.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2882f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|words_nonempty|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|             i|\n",
      "|           for|\n",
      "|           use|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|             a|\n",
      "|          cost|\n",
      "|           and|\n",
      "|          with|\n",
      "|        almost|\n",
      "|  restrictions|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_words = [\"is\", \"not\", \"if\", \"the\"]\n",
    "words_filtered = words_nonempty.filter(~words_nonempty.words_nonempty.isin(filter_words))\n",
    "words_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e33009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|words_nonempty|count|\n",
      "+--------------+-----+\n",
      "|             i| 5146|\n",
      "|           and| 3046|\n",
      "|             a| 2676|\n",
      "|          that| 1033|\n",
      "|           was| 1022|\n",
      "|          with|  714|\n",
      "|           but|  692|\n",
      "|           had|  686|\n",
      "|           you|  644|\n",
      "|         which|  565|\n",
      "|           his|  535|\n",
      "|           for|  524|\n",
      "|          this|  449|\n",
      "|          from|  400|\n",
      "|           her|  373|\n",
      "|          have|  370|\n",
      "|          when|  329|\n",
      "|          were|  308|\n",
      "|          your|  261|\n",
      "|           she|  255|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------------+\n",
      "|count(words_nonempty)|\n",
      "+---------------------+\n",
      "|                 7208|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_filtered.groupBy(\"words_nonempty\").count().sort(\"count\", ascending=False).show()\n",
    "words_filtered.agg(F.countDistinct(\"words_nonempty\")).show()\n",
    "# words_nonempty.coalesce(1).write.csv(\"hallu2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b402fb3",
   "metadata": {},
   "source": [
    "## Die gro√üe Frage: In welcher Reihenfolge filtern? - Wie crazy muss die Regex sein?\n",
    "Antwort: Lesbarkeit geht vor!</br>Spark evaluiert lazy und kann \"hintendran\" sehr viel optimieren.</br>Es gibt keine Reihenfolge, da man es so durchf√ºhren sollte, wie man es selbst am Besten versteht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d06f4d",
   "metadata": {},
   "source": [
    "## Aufgabe 1\n",
    "Das Wort \"is\" soll aus dem gesamten Text entfernt werden und W√∂rter der mindestl√§nge von 3 sollen ausgegeben werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dca25102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|words_nonempty|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|            by|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|            of|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|            at|\n",
      "|            no|\n",
      "|          cost|\n",
      "|           and|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_short = words_nonempty.where(col(\"words_nonempty\") != \"is\")\n",
    "words_short.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea58a9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|words_nonempty|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|          cost|\n",
      "|          with|\n",
      "|        almost|\n",
      "|  restrictions|\n",
      "|    whatsoever|\n",
      "|          copy|\n",
      "|          give|\n",
      "|          away|\n",
      "|         under|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "min3Zeichen = words_nonempty.where(length(col(\"words_nonempty\"))>3)\n",
    "min3Zeichen.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee10906",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "Finde programmatisch heraus, wie viele Spalten **keine** Strings sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2a7cb73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: string (nullable = true)\n",
      " |-- 2: string (nullable = true)\n",
      " |-- 3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datenA2 = spark.createDataFrame([[\"test\", \"noch ein test\", 10_000_000_000]],[\"1\",\"2\",\"3\"])\n",
    "datenA2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d312ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt = 1\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for x, y in datenA2.dtypes:\n",
    "    if y != \"string\":\n",
    "        cnt += 1\n",
    "print(f\"cnt = {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ad31b",
   "metadata": {},
   "source": [
    "## Aufgabe 3\n",
    "Mache den Code lesbarer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "63488b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgangscode\n",
    "datenA3 = spark.read.text(\"../frankenstein.txt\").select(length(col(\"value\"))).withColumnRenamed(\"length(value)\", \"numChar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "82522857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Austauschen von withColumnRenamed mit alias()\n",
    "datenA3 = spark.read.text(\"../frankenstein.txt\").select(length(col(\"value\"))).alias(\"numChar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad541b8",
   "metadata": {},
   "source": [
    "## Aufgabe 4\n",
    "Im folgenden Code gibt es ein Problem. Was ist es und wie kann man es reparieren?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "96441ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value1: long (nullable = true)\n",
      " |-- value2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datenA4 = spark.createDataFrame([[\"key\", 20_000_000, 10_000_000_000]], [\"key\", \"value1\", \"value2\"])\n",
    "datenA4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd4dfeaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'key' does not exist. Did you mean one of the following? [maxVal];\n'Project ['key, maxVal#489L]\n+- Project [greatest(value1#470L, value2#471L) AS maxVal#489L]\n   +- LogicalRDD [key#469, value1#470L, value2#471L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m greatest\n\u001b[0;32m----> 3\u001b[0m datenA4M \u001b[38;5;241m=\u001b[39m \u001b[43mdatenA4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgreatest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxVal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxVal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m datenA4M\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'key' does not exist. Did you mean one of the following? [maxVal];\n'Project ['key, maxVal#489L]\n+- Project [greatest(value1#470L, value2#471L) AS maxVal#489L]\n   +- LogicalRDD [key#469, value1#470L, value2#471L], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "\n",
    "datenA4M = datenA4.select(greatest(col(\"value1\"), col(\"value2\")).alias(\"maxVal\")).select(\"key\", \"maxVal\")\n",
    "datenA4M.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e78a72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|key|     maxVal|\n",
      "+---+-----------+\n",
      "|key|10000000000|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "\n",
    "datenA4M = datenA4.select(col(\"key\"),greatest(col(\"value1\"), col(\"value2\")).alias(\"maxVal\")).select(\"key\", \"maxVal\")\n",
    "datenA4M.show()\n",
    "\n",
    "# Das Problem bestand darin, dass ein select auf den \"key\" get√§tigt wurde, allerdings diese Spalte noch nicht existierte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df811658",
   "metadata": {},
   "source": [
    "## Aufgabe 5\n",
    "Filtere mit Hilfe der Funktion *isin* die W√∂rter *is*, *not*, *if* und *the* aus dem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8fe01101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|words_nonempty|\n",
      "+--------------+\n",
      "|       project|\n",
      "|     gutenberg|\n",
      "|  frankenstein|\n",
      "|            by|\n",
      "|          mary|\n",
      "|wollstonecraft|\n",
      "|        godwin|\n",
      "|       shelley|\n",
      "|          this|\n",
      "|         ebook|\n",
      "|           for|\n",
      "|           use|\n",
      "|            of|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|            at|\n",
      "|            no|\n",
      "|          cost|\n",
      "|           and|\n",
      "|          with|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_words = [\"is\", \"not\", \"if\", \"the\"]\n",
    "words_nonempty.filter(~words_nonempty.words_nonempty.isin(filter_words)).show()\n",
    "#words_nonempty.filter(words_nonempty.words_nonempty.isin(filter_words)).show() = Gibt alle filter_words aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fe9c8a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|filter word|\n",
      "+-----------+\n",
      "|         is|\n",
      "|        not|\n",
      "|         if|\n",
      "|        the|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filter_words = spark.createDataFrame([[\"is\"], [\"not\"], [\"if\"], [\"the\"]],[\"filter word\"])\n",
    "df_filter_words.show()\n",
    "\n",
    "#df2_filter_words = spark.createDataFrame([[\"is\", \"not\", \"if\", \"the\"]],[\"filter word\"])\n",
    "#df2_filter_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef22c3",
   "metadata": {},
   "source": [
    "## Aufgabe 6\n",
    "Finde den / die Fehler im folgenden Code und repariere ihn so, dass der Code so wie erwartet funktioniert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "930af5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path does not exist: file:/home/jovyan/spark/frankenstein.txt\n",
      "+--------------+\n",
      "|          word|\n",
      "+--------------+\n",
      "|              |\n",
      "|       Project|\n",
      "|   Gutenberg's|\n",
      "| Frankenstein,|\n",
      "|            by|\n",
      "|          Mary|\n",
      "|Wollstonecraft|\n",
      "|      (Godwin)|\n",
      "|       Shelley|\n",
      "|              |\n",
      "|          This|\n",
      "|         eBook|\n",
      "|            is|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|            of|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|            at|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "try:\n",
    "    book = spark.read.text(\"frankenstein.txt\")\n",
    "    book = book.printSchema() # √úberschreiben der Variable book -> Select kann auf das Schema nicht ausgef√ºhrt werden!\n",
    "    lines= book.select(spolit(book.value, \" \").alias(\"lime\")) # spolit = split\n",
    "    words = lines.select(explode(col(\"line\")).alias(\"word\")) # Spalte \"line\" existiert nicht\n",
    "except AnalysisException as err:\n",
    "    print(err)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0584ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------+\n",
      "|          word|\n",
      "+--------------+\n",
      "|              |\n",
      "|       Project|\n",
      "|   Gutenberg's|\n",
      "| Frankenstein,|\n",
      "|            by|\n",
      "|          Mary|\n",
      "|Wollstonecraft|\n",
      "|      (Godwin)|\n",
      "|       Shelley|\n",
      "|              |\n",
      "|          This|\n",
      "|         eBook|\n",
      "|            is|\n",
      "|           for|\n",
      "|           the|\n",
      "|           use|\n",
      "|            of|\n",
      "|        anyone|\n",
      "|      anywhere|\n",
      "|            at|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "try:\n",
    "    text = spark.read.text(\"../frankenstein.txt\")\n",
    "    book = text.printSchema() \n",
    "    lines= text.select(split(text.value, \" \").alias(\"lime\"))\n",
    "    words = lines.select(explode(col(\"lime\")).alias(\"word\"))\n",
    "except AnalysisException as err:\n",
    "    print(err)\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d938e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
